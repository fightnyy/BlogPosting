1. 안녕하세요  2021 모두콘에서  "챗봇의 윤리문제와 방안" 이란 주제로 발표를 진행하게 된  나영윤입니다. 

2. 현재 저는 튜닙에서 NLP engineer 로 일하고 있습니다.  현재 회사에서는 [1] 윤리모델과 관련된 업무를 진행하고 있습니다. 많은 분들께서 트랜스포머와 같은 NLP 모델링에 대해서는 익히 들어보셨겠지만 윤리문제는 그렇게 많이 들어보시지 않으셨을거 같아요. 그래서 처음 발표를 보실 때 

3. 이게 어떤거지? 란 생각으로 들어오셨을거 같습니다. 개인적인 생각으론 윤리문제도  NLP 모델링 만큼 상당히 중요한 부분을 차지 한다고 생각합니다. 이번 기회에 저와 같이 윤리문제는 어떤걸 다루고 어떤 것인지  함께 알아보는 시간이 되었으면 합니다.

4. 발표를 본격적으로 시작하기 앞서 제가 인상깊게 봤던 만화 얘기를 먼저하고 시작해도 좋을 거 같습니다. 이 만화를 아시는 분들도 여럿 계실 거 같은데요. 위 만화는 xkcd란 만화가가 언어를 주제로 그린 만화입니다. 만화의 핵심은 Language 즉, 언어란 정규화된 시스템이 아니라 아름다운 카오스라는 것입니다. 문맥, 화자, 청자등 여러가지 요인에 따라서 언어는 여러가지로 해석될 수 있다는 것입니다. 

5. 지금 제 말을 듣고 여러분의 표정은 아마 이런 표정과 비슷할 거 같은데요

6.  사실 이 그림이 제가 오늘 말하고자 하는 주제와 매우 연관 깊습니다. 자연어 처리에서 윤리문제는 어떤것을 담당하는 걸까요? 윤리 문제는 그 범위에 따라서 여러가지로 나눠볼 수 있겠지만 저는 그중에서  hate speech에 대해 주로 말씀드리고자 합니다.

7. hate speech는 지금 보이는 사진과 같이 챗봇이 욕설, 차별 및 혐오 표현과 같은 발화를 하는 것을 얘기합니다. 아래의 예시에서 인종과 임산부석에 대해서 부정적으로 얘기하고 성소수자, 장애인 에 대해서도 혐오, 차별적인 응답으로 발화하고 있는 챗봇의 모습을 볼 수 있습니다. 저희가 챗봇에게 기대하는 것은 이런 발화가 아닙니다. 저희가 원하는 것은 재미있지만 즉, 충분히 사람처럼 얘기하지만 편견이나 혐오적인 표현을 하지 않는 발화를 기대합니다. 

8. 이문제를 어떻게  해결 할 수 있을까요 한번 곰곰히 생각해 보죠.몇가지 아이디어가 떠오르셨을거 같아요. 안떠오르셔도 괜찮습니다. 이것만 보고 떠올리셨다면 그게오히려 대단한거죠. 우선 위와 같은 Hate speech를 완화하기 위해 연구되었던 논문을 한번 살펴볼까요?

9. 첫쨰로 2019년 페이스북이 발표한 Build it Break it Fix it for Dialogue Safety 논문입니다. 

10. 먼저 그림으로 설명해 드리자면 [1] 기존에 존재하는 데이터를 사용하여 윤리 분류기를 만듭니다. 그렇게 해서 나온 모델이 A0입니다. 이제 이 A0라는 분류기를 놓고 [2]사람이 공격적인 발화를 합니다. 만약 여기서 분류기가 사람의 발화를 잘 분류를 했다면  [3] OFFENSIVE라고 분류를 할 것이고 만약 SAFE라고 분류를 했다면 이는 잘못된 분류를 한것입니다. 그럼 이 잘못분류한 즉, 실제로는 Offensive한 말이지만 분류기는 [4]Safe로 표현한 이 새로운 데이터셋을 또 분류기에 학습시킵니다. 이렇게 함으로써 모델은 점점 더 robust한 결과를 나타낼 것입니다.

11. 자 그럼 저희도 이 아이디어를 가지고 한번 응용해보죠.  이 빨간색 네모 부분을 윤리 모델 분류기에 넣고 만약 혐오 표현을 한다고 분류를 하면 "음 그런말은 하지말자", 라던지 "대화 주제로 이건 어때?" 라는 형식으로 응용해볼 수 있을 거 같아요. 또한 이런 데이터를 안전하다고 즉, safe 하다고 분류 한 데이터를 추가하여 분류기를 더욱 robust하게 학습시키는 것도 추가하면 좋겠네요

12. 그림으로 설명하자면 이렇게 표현 될 수 있을 거 같아요 "그럼 그냥 죽는거지 어떤 고민도 선택도 없는 대신 고통도 없을 거 같음" 이라는  speech에 대해서 윤리모델 분류기가 safe 또는 offensive로 판단 하여 safe 하다고 판별하면 원래 챗봇이 발화하려고 했던 문장 그대로 발화를 하도록 하고 그렇지 않으면 "그 주제 말고 우리 다른 주제 얘기해볼까?" 라는 형식으로 주제를 우회하는 방식입니다.

13. 자 이제 됐습니다. 윤리문제 생각보다 별거 없죠. 이제 윤리 문제는 다 해결 됐으니 챗봇의 말을 어떻게 하면 더 잘 하게 만들지에 대해 연구하면 될 거 같아요.

14. 흠 그런데 사용자들의 실제로 대화한 예시를 한번 주의깊게 보죠. 흰색 부분이 챗봇이 말한 부분입니다. 챗봇이 말한 부분을 보면 챗봇은 전혀 공격적인 발화를 하지 않은것을 볼 수 있습니다. 다만, 문맥 상 챗봇의 발화가 문제가 될 소지가 상당히 많아 보이죠.

15. 이렇게 하므로써 저희가 내릴 수 있는 결론은 챗봇의 문장하나, 즉, 싱글턴으로는 문제가 될만한 발화를 잡는데 많은 한계점이 있구나라는 점입니다. 따라서 문장 하나만 보는 것이 아닌 전체적인 문맥을 보는, 즉 멀티턴으로 챗봇의 문제있는 발화를 잡아야 우리가 경계하는 문제있는 발화를 더욱 더 완화 시킬 수 있겠구나 라는 생각을 할 수 있습니다. 이제 제가 처음에 보여드렸던 만화가 이해 되시나요? 

16. 그럼 이제 이 문제를 어떻게 해결해볼 수 있을까요? 물론 당연히 정답은 없습니다. 이와 관련된 논문을 한번 더 살펴 보죠. 이 논문은 Recipes for Safety in Open-domain Chatbots 이라는 제목으로 페이스북에서 발표한 논문입니다.

17. 기존의 한계가 뚜렸한 싱글턴 문제를 어떻게 멀티턴으로 잡아낼 수 있을까요? 우선 [1]첫번째 블렌더 봇같은 챗봇 모델을 만들기 위해 기존에 존재하는 데이터 예를들어 blended skill talk 같은 데이터로 챗봇 모델을 학습시킵니다. 그럼 이제 [2]사람이 챗봇과 대화를 나누면서 아까처럼 직접적으로 Offensive 한 발화 또는 직접적으로 Offensive한 것은 아니지만 문맥적으로 Offensive 한 발화를 합니다. 만약 이때 [3] 윤리 분류기가 이 대화 세션에 대해 safe로 분류했다면 이는 잘못 분류한것입니다. 그럼 이제 새롭게 얻는 즉, 기존의 윤리 분류기가 잘 판별하지 못했던  [4]Adversarial data를 다시 윤리 분류기에 학습시켜 이제 멀티턴에서도 robust한 윤리 분류기가 나올 수 있게 된거죠.

18. 그럼 멀티턴까지 해결했으니 hate speech는 완전히 종결된 것이네요 다행이네요 그래도 다 해결책이 있어서.

19. 이미 예상 하셨겠지만 이 역시 전혀 아닙니다. 이번  7월에 Blender Bot 2.0이 나왔습니다. 여기 제가 인용한 문장들은  blender bot 2.0 에서 safety 이슈에 결론을 가져온것입니다. 결론은 현재 제가 말씀드렸던 기법과 같은 여러가지 노력에도 불구하고 safety 이슈는 아직 해결 되지 않았다는 의미입니다. 물론 챗봇이 offensive한 발화를 하는것이 많이 줄어들었지만 그렇다고 완전히 하지 않는것도 아니고 blender bot 2.0에서는 기존의 챗봇과 다르게 챗봇이 발화하기전에 인터넷 검색과 장기기억을 다루는 모듈이 추가 되었기 때문에 새로운 safety issue가 있을 수 있다는 점을 시사하고 있습니다. 참고로 제가 blender bot 2.0도 리뷰를 한번 했었는데요 아래 링크로 들어가시면 blenderbot 2.0의 자세한 리뷰를 볼 수 있습니다. 

20. 그럼 현재 저희는 어떠한 방향으로 윤리문제를 해결하려고 할까요?

21. 저희는 챗봇의 문제되는 발화를 완화하기 위해  더욱 세부적인 분류체계를 사용합니다. 유해성을 분석하기 위해 이 문장에 Offensive 한 문장인데 그 중에서 어떤 분류체계에 들어가는지 또 심각성은 어느정도인지를 확인합니다. 또한 챗봇의 개인정보 발화 문제도 다룹니다. 현재 챗봇의 또다른 이슈는 챗봇의 학습데이터로 개인정보가 들어가 이를 발화한다는 문제점이 있습니다. 이를 해결하기 위해 개인정보 검출 모듈을 새로 제작하였고 정규식과 데이터베이스 그리고 NER 모델을 활용하여 이를 미리 검출합니다. 마지막으로 챗봇이 사람과 대화를 할 때 대화하기 민감할 수 있는 주제를 선정하였고 해당 주제를 판별할 수 있는 민감주제 분류모델도 제작중에 있습니다. 이번 발표에서는 민감주제와, 개인정보 모듈까지 얘기를 하면 발표가 길어지니 유해성 분석 모듈을 주로 말씀드리도록 하겠습니다.

22. 저희는 유해성을 본격적으로 분류하기위해 혐오표현을 종교, 민족성, 국적, 인종,성별 등의 정체성 요인을 이유로 개인이나 집단을 멸시, 모욕, 위협하거나 그들의 차별, 적의, 폭력을 선동하는 것으로 정의하였습니다. 하지만 이런 혐오표현도 세분화 시킬 수 있다고 판단하였고

23. 더 정확한 판별을 위해 8개의 정체성 요인으로 혐오 표현을 세분화 하였습니다.

24. 또. 욕설, 외설 등과 같은 유해성이 짙은 표현을 추가하여 총 13개의 속성으로 유해성을 판단합니다.그렇다면 유해성의 심각정도는 어떨까요? 

25. 한번 생각해보죠 만약 사용자가 아 밥도 다 먹었으니까 시원하게 민쵸나 먹어야지란 발화를 했을때 이에 대한 응답으로 챗봇이 나는 반 민쵸파야 민트초코 왜 먹는지 모르겠네. 먹는사람 이해가 안돼 와 같은 발화를 한다고 했을떄 이는 혐오표현이라고 할 수있을까요? 

26. 또, 나는 반 민쵸파야 민트초코 먹는 새끼들 이해가 안돼 다 죽었으면. 이라는 표현을 할땐 어떤 가요? 

27. 저희는 이러한 문제를 해결하기 위해 심각도의 scale을 총 0부터 4까지 5단계로 나눠 속성별 유해성을 판별하습니다. 0이 가장 낮은 것을 의미하고 4가 가장 강한 것을 의미합니다. 1과 2로 분류된 것은 주로 간접적인 표현을 의미하고 3과 4로 분류된것은 매우 직집적으로 공격성이 짙은 표현으로 분류 하여 데이터를 모았습니다.

28. 저희는 어떤 문장안에 어떤 표현이 문제성이 짙은지 표현하기 위해 이를 별도의 하이라이트 표시를 하였습니다. 아래의 예시를 보면 "ㅈㄹ 염병" ㅋㅋ 재산이 1조인데 세금 안내려고 머리 굴리다 벌 받은거지 부분에서 "ㅈㄹ 염병" 을 하이라이트로 표시하여 사용자에게 어디 부분에서 해당 문장이 유해성을 띈 문장인지를 보여 줄 수 있습니다.

29. 이 모든걸 기초하기 위해선 당연히 데이터가 많이 필요합니다. 저희는 한국어 약 16만 문장 데이터를 라벨링을 했고 출처는 아래와 같이 BEPP 한국어 오픈 데이터셋, 포털 뉴스 댓글, 온라인 커뮤니티 댓글을 활용했습니다. 또한 영어도 약 5만 문장정도 라벨링을 하였습니다. 이 라벨링 관점에서 아무래도 사람이 라벨링을 직접 진행하다보니 저희가 사전교육을 했음에도 불구하고 사람마다 기준의 척도가 일부 주관적이라는 문제점이 있었습니다. 따라서 이를 해결 하기 위해 라벨러간의 크로스체크를 진행했습니다.

30. 크로스 체크는 한 문장에 대해서 라벨러간의 점수 편차가 2점이 초과된다면 해당 데이터는 버리고 점수차이가 2점 이하인 것들만 살리는 형식으로 데이터 라벨링을 진행했습니다. 또한 2점 이내라고 하여도 만약 점수차이가 1점이면 둘 중에 더 점수가 큰것 만약 점수차이가 2점이면 두개의 평균을 내어 라벨링을 진행했습니다. 밑에 그림의 예시를 보죠. [1] 어떤 문장에 대해서 라벨러 A가 욕설에 4점을 주었고 라벨러 B가 욕설에 3점을 주었다면 최종 라벨은 4점으로 기록됩니다. [2] 또 어떤 문장에 대해서 라벨러 A가 욕설에 4점을 주고 라벨러 B는 욕설에 2점을 주었다면 이둘의 평균이 3이 최종 라벨 결과가 됩니다.

31. 그렇지만 이렇게 데이터를 버리고 나니 데이터 중 장애 혐오 문장 데이터가 부족하다는 것을 발견했고 이를 해결하기 위해 디씨인사이드, 네이버 등과 같은 온라인 커뮤니티에서 데이터를 추가로 수집하여 부족했던 데이터를 어느정도 해결 할 수 있었습니다.

32. 마지막으로 저희는 데이터의 pre train 단계에서 혐오 표현과 같은 발화를 하지 않도록 raw 데이터를 윤리모델에 거쳐 안전한 데이터로 만드는 전처리 작업 기능도 진행하고 있습니다. 기존에 사용했던 데이터셋을 전처리 모델에 넣어 윤리적이지 않은 문장을 제거하고 모델의 pre-train을 진행하는 것이죠.  여기서 저희는 하나 더 고민을 해야 했습니다. 바로 포맷이죠 모델 데이터의 전처리 작업을 수행하기 위해선 데이터를 문단으로 넣을지 아니면 문장으로 넣을지 고민을 해야합니다. 설명을 더 용이하게 하기 위해 그림을 한번 보죠. 

33.  아래와 같이 한 문단이 있다고 가정해보면 이를 문장으로 나눕니다. 해당 문단 중에서 문장으로 나누었을 때 몇개의 문장 "선생님들은 아이들에게 욕을 쓰면 안된다고 가르쳤다.", "이런말을 절대 쓰면 안되는 말이란다. 꼭 주의해주길 바라" 와 같은 문장은  안전한 문장이기 때문에 모델의 학습 데이터로 사용하고 그렇지 않은 "철수는 바보, 멍청이다." 라는 것은 학습데이터로 사용하지 않는 형태입니다.  참고로 문장 나누기 모듈은 저희 회사의 고현웅님이 관리하고 계신 KSS를 사용하면 일반적인 sentence spliter 보다 훨씬 고품질로 문장을 나눠 더욱 좋은 품질의 데이터를 만들 수 있습니다.

34. 두번째로는 문단 전체를 딥러닝 모델에 넣어 이것이 안전한지 안전하지 않은지 판별하는 형태가 있을 수 있습니다. 이렇게 하면 전체적인 문맥을 보고 판단하기 때문에 좀 더 서비스에 적합한 형식으로 판별이 가능합니다. 

35. 이상 저와 같이 챗봇의 윤리문제와 그 방안에 대해 알아보았습니다.  발표 듣느라 정말 고생하셨습니다

36. .저 역시 맨처음 이 프로젝트를 시작할때 윤리 문제가 이렇게 까지 할게 많나라는 생각이었지만 아직도 보완해야 할점도 많다고 생각합니다.

37. 발표에 더 말씀드리고 싶은 부분도 있었고 궁금하신 사항도 많으실거란 생각도 듭니다. 이에 대한 질문사항은 아래 발표자 메일로 질문 주시면 답변드리도록 하겠습니다. 또한 튜닙은 아래와 같이 다양한 부문에서 동료를 찾고 있습니다. 튜닙은 채용이라는 말 대신 영입이라는 말을 씁니다. 사람이 전부인 스타트업에서 인재는 '뽑는' 사람이 아니라, '같이하고픈' 예비동료이기 때문입니다.

     자세한 내용은 튜닙의 홈페이지([https://tunib.ai](https://tunib.ai/))를 참조해 주세요.감사합니다. 이상 튜닙의 NLP 엔지니어 나영윤이었습니다.





